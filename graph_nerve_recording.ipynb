{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOFkUxTIYvBj",
        "outputId": "017ebc67-8b6c-41ef-dfe4-8cf641fde6b4"
      },
      "outputs": [],
      "source": [
        "# Construct Training/Evaluation Sets\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import spektral\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def load_data_by_rat(processed_path, selected_rats, training):\n",
        "    X, Y = [], []\n",
        "    for file in os.listdir(processed_path):\n",
        "      if training == True:\n",
        "        if any(rat in file for rat in selected_rats) and file.endswith('fold0.npz'):\n",
        "            print(f\"Loading processed file: {file}\")\n",
        "            data = np.load(os.path.join(processed_path, file))\n",
        "            X.append(data['X'])\n",
        "            Y.append(data['Y'])\n",
        "      else:\n",
        "        if any(rat in file for rat in selected_rats) and file.endswith('fold0.npz'):\n",
        "            print(f\"Loading processed file: {file}\")\n",
        "            data = np.load(os.path.join(processed_path, file))\n",
        "            X.append(data['X'])\n",
        "            Y.append(data['Y'])\n",
        "\n",
        "    X = np.concatenate(X, axis=0)\n",
        "    Y = np.concatenate(Y, axis=0)\n",
        "    return X, Y\n",
        "\n",
        "# Split rats into train/test (generalizability evaluation)\n",
        "test_rat = [\"Rat5\"] \n",
        "train_rat = [\"Rat2\", \"Rat4\", \"Rat5\", \"Rat7\", \"Rat8\", \"Rat9\", \"Rat10\"]\n",
        "\n",
        "\n",
        "processed_path = '/content/drive/MyDrive/Graphs/ProcessedGraphs'\n",
        "\n",
        "# Load data\n",
        "X_train, Y_train = load_data_by_rat(processed_path,train_rat, training=True)\n",
        "X_test, Y_test = load_data_by_rat(processed_path,test_rat, training=False)\n",
        "\n",
        "\n",
        "# Print shapes for verification\n",
        "print(f\"Train: {X_train.shape}, {Y_train.shape}\")\n",
        "print(f\"Test: {X_test.shape}, {Y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YAd_aDsqYvnW"
      },
      "outputs": [],
      "source": [
        "# Geodesic Distance Graph Construction\n",
        "from spektral.data import Dataset, Graph\n",
        "import spektral\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "def geodesic_distance(channel1, channel2):\n",
        "    r1, c1 = divmod(channel1, 8)  # 8 channels per ring\n",
        "    r2, c2 = divmod(channel2, 8)\n",
        "\n",
        "    h_dist = min(abs(c1 - c2), 8 - abs(c1 - c2))\n",
        "    v_dist = abs(r1 - r2)\n",
        "\n",
        "    return h_dist + v_dist\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, ele, nei, distance_threshold, X, Y, **kwargs):\n",
        "\n",
        "        self.nei = nei\n",
        "        self.ele = ele\n",
        "        self.a = None\n",
        "        self.distance_threshold = distance_threshold\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def read(self):\n",
        "\n",
        "        x = self.X\n",
        "        self.a = self.geodesic_adj(self.nei, self.ele)\n",
        "        return [Graph(x=x_, y=y_) for x_, y_ in zip(self.X, self.Y)]\n",
        "\n",
        "    def geodesic_adj(self, nei, num_elec):\n",
        "        sigma = 2\n",
        "        geodesic_matrix = np.zeros((num_elec, num_elec))\n",
        "\n",
        "        for i in range(num_elec):\n",
        "            for j in range(num_elec):\n",
        "                geodesic_matrix[i, j] = geodesic_distance(i, j)\n",
        " \n",
        "\n",
        "        sorted_indices = np.argsort(geodesic_matrix, axis=1)  \n",
        "        knn_indices = sorted_indices[:, 1:nei+1]  \n",
        "\n",
        "        adjacency_matrix = np.zeros_like(geodesic_matrix)\n",
        "        for i in range(num_elec):\n",
        "            adjacency_matrix[i, knn_indices[i]] = np.exp(- (geodesic_matrix[i, knn_indices[i]] ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "        return adjacency_matrix.astype(float)\n",
        "\n",
        "data_tr = MyDataset(56, 5, 5, X_train, Y_train)\n",
        "data_te = MyDataset(56, 5, 5, X_test, Y_test)\n",
        "\n",
        "data_tr.a = spektral.utils.sparse.sp_matrix_to_sp_tensor(\n",
        "    spektral.layers.EdgeConv.preprocess(data_tr.a)\n",
        ")\n",
        "\n",
        "data_te.a = spektral.utils.sparse.sp_matrix_to_sp_tensor(\n",
        "    spektral.layers.EdgeConv.preprocess(data_te.a)\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hdmY3FfOmIVM"
      },
      "outputs": [],
      "source": [
        "# Random Graph Construction\n",
        "\n",
        "from spektral.data import Dataset, Graph\n",
        "import spektral\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def random_adj(num_nodes, max_degree=2, seed=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    adj = np.zeros((num_nodes, num_nodes))\n",
        "    degrees = np.zeros(num_nodes, dtype=int)\n",
        "\n",
        "    possible_edges = [(i, j) for i in range(num_nodes) for j in range(i+1, num_nodes)]\n",
        "    np.random.shuffle(possible_edges)\n",
        "\n",
        "    for i, j in possible_edges:\n",
        "        if degrees[i] < max_degree and degrees[j] < max_degree:\n",
        "            adj[i, j] = adj[j, i] = 1\n",
        "            degrees[i] += 1\n",
        "            degrees[j] += 1\n",
        "\n",
        "    return adj\n",
        "\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, ele, max_degree, X, Y, **kwargs):\n",
        "        self.ele = ele\n",
        "        self.max_degree = max_degree\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.a = None\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def read(self):\n",
        "        self.a = self.random_adj(self.ele, self.max_degree)\n",
        "        return [Graph(x=x_, y=y_) for x_, y_ in zip(self.X, self.Y)]\n",
        "\n",
        "    def random_adj(self, num_nodes, max_degree):\n",
        "        return random_adj(num_nodes, max_degree)\n",
        "\n",
        "\n",
        "\n",
        "data_tr = MyDataset(56, max_degree=2, X=X_train, Y=Y_train)\n",
        "data_te = MyDataset(56, max_degree=2, X=X_test, Y=Y_test)\n",
        "\n",
        "data_tr.a = spektral.utils.sparse.sp_matrix_to_sp_tensor(\n",
        "    spektral.layers.EdgeConv.preprocess(data_tr.a)\n",
        ")\n",
        "\n",
        "data_te.a = spektral.utils.sparse.sp_matrix_to_sp_tensor(\n",
        "    spektral.layers.EdgeConv.preprocess(data_te.a)\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GdEJb2zKZ4xE"
      },
      "outputs": [],
      "source": [
        "# Euclidean Graph Construction\n",
        "\n",
        "from spektral.data import Dataset, Graph\n",
        "import spektral\n",
        "import tensorflow as tf\n",
        "\n",
        "def electrode_coordinates(num_elec=56, n_per_ring=8,\n",
        "                           ring_spacing_mm=3.33,\n",
        "                           radius_mm=0.5):\n",
        "    \n",
        "    coords = np.zeros((num_elec, 2))\n",
        "\n",
        "    for i in range(num_elec):\n",
        "        r, c = divmod(i, n_per_ring)\n",
        "\n",
        "        theta = 2 * np.pi * c / n_per_ring\n",
        "        x = radius_mm * np.cos(theta)\n",
        "        y = r * ring_spacing_mm \n",
        "\n",
        "        coords[i] = [x, y]\n",
        "\n",
        "    return coords\n",
        "\n",
        "def euclidean_distance_matrix(coords):\n",
        "    diff = coords[:, None, :] - coords[None, :, :]\n",
        "    return np.linalg.norm(diff, axis=-1)\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, ele, nei, distance_threshold, X, Y, **kwargs):\n",
        "        self.nei = nei\n",
        "        self.ele = ele\n",
        "        self.a = None\n",
        "        self.distance_threshold = distance_threshold\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "        self.coords = electrode_coordinates(num_elec=ele)\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def read(self):\n",
        "        self.a = self.euclidean_adj(self.nei, self.ele)\n",
        "        return [Graph(x=x_, y=y_) for x_, y_ in zip(self.X, self.Y)]\n",
        "\n",
        "    def euclidean_adj(self, nei, num_elec):\n",
        "        sigma = 2.0\n",
        "\n",
        "        dist_matrix = euclidean_distance_matrix(self.coords)\n",
        "\n",
        "        sorted_indices = np.argsort(dist_matrix, axis=1)\n",
        "        knn_indices = sorted_indices[:, 1:nei + 1]\n",
        "\n",
        "        adjacency_matrix = np.zeros_like(dist_matrix)\n",
        "        for i in range(num_elec):\n",
        "            d = dist_matrix[i, knn_indices[i]]\n",
        "            adjacency_matrix[i, knn_indices[i]] = np.exp(\n",
        "                - (d ** 2) / (2 * sigma ** 2)\n",
        "            )\n",
        "\n",
        "        return adjacency_matrix.astype(float)\n",
        "\n",
        "data_tr = MyDataset(56, 5, 5, X_train, Y_train)\n",
        "data_te = MyDataset(56, 5, 5, X_test, Y_test)\n",
        "\n",
        "data_tr.a = spektral.utils.sparse.sp_matrix_to_sp_tensor(\n",
        "    spektral.layers.EdgeConv.preprocess(data_tr.a)\n",
        ")\n",
        "\n",
        "data_te.a = spektral.utils.sparse.sp_matrix_to_sp_tensor(\n",
        "    spektral.layers.EdgeConv.preprocess(data_te.a)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7bcctk6ZldB",
        "outputId": "4183ae0b-0c38-47d3-ca84-4a32ccb491fb"
      },
      "outputs": [],
      "source": [
        "# Model Training\n",
        "import tensorflow as tf\n",
        "import spektral\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 256 \n",
        "epochs = 2000  \n",
        "patience = 100  \n",
        "l2_reg = 5e-3 \n",
        "learning_rate = 1e-3\n",
        "\n",
        "loader_te = spektral.data.MixedLoader(data_te, batch_size=batch_size)\n",
        "\n",
        "def gaussian_blur(image, kernel_size, sigma, padding='SAME'):\n",
        "  radius = tf.cast(kernel_size / 2, dtype = tf.int32)\n",
        "  kernel_size = radius * 2 + 1\n",
        "  x = tf.cast(tf.range(-radius, radius + 1), dtype = tf.float32)\n",
        "  blur_filter = tf.exp(\n",
        "      -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.cast(sigma, dtype = tf.float32), 2.0)))\n",
        "  blur_filter /= tf.reduce_sum(blur_filter)\n",
        "  # One vertical and one horizontal filter.\n",
        "  blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
        "  blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
        "  num_channels = tf.shape(image)[-1]\n",
        "  blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
        "  blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
        "  expand_batch_dim = image.shape.ndims == 3\n",
        "  if expand_batch_dim:\n",
        "    image = tf.expand_dims(image, axis=0)\n",
        "  blurred = tf.nn.depthwise_conv2d(\n",
        "      image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n",
        "  blurred = tf.nn.depthwise_conv2d(\n",
        "      blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n",
        "  if expand_batch_dim:\n",
        "    blurred = tf.squeeze(blurred, axis=0)\n",
        "  return blurred\n",
        "\n",
        "\n",
        "class Augmentation_layer1(tf.keras.layers.Layer):\n",
        "    def __init__(self, N, M):\n",
        "        super(Augmentation_layer1, self).__init__()\n",
        "        self.M = M\n",
        "        self.N = N\n",
        "\n",
        "    def Trans(self, image, count):\n",
        "        image = tf.reshape(image, (image.shape[0], image.shape[1], -1))\n",
        "        image = tf.stack([image, image, image], axis=-1)\n",
        "        image = tf.convert_to_tensor(image)\n",
        "\n",
        "        G = tf.random.uniform(shape=[], minval=0, maxval=0.1, dtype=tf.float32) #Gaussian noise\n",
        "        image = tf.keras.layers.GaussianNoise(G)(image)\n",
        "\n",
        "        image = image[:, :, :, 0]\n",
        "        return image\n",
        "\n",
        "    def call(self, image, training):\n",
        "        image = tf.cast(image, dtype=tf.float32)\n",
        "\n",
        "        if training:\n",
        "            for _ in range(self.M):\n",
        "                image = self.Trans(image, None)\n",
        "\n",
        "        return tf.cast(image, dtype=tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Net(tf.keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.A = Augmentation_layer1(3, 3) \n",
        "        self.C1 = tf.keras.layers.Conv1D(64, kernel_size=batch_size, activation = 'relu', padding='same')\n",
        "        self.L1 =tf.keras.layers.LSTM(256, activation='relu',return_sequences=True) \n",
        "\n",
        "        self.conv1 = spektral.layers.GeneralConv(channels=128) \n",
        "        self.conv2 = spektral.layers.EdgeConv(32, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)) #32\n",
        "\n",
        "        self.b1 = tf.keras.layers.BatchNormalization()\n",
        "        self.b2 = tf.keras.layers.BatchNormalization()\n",
        "        self.d3 = tf.keras.layers.Dropout(0.2) #0.2\n",
        "        self.d4= tf.keras.layers.Dropout(0.2)\n",
        "        self.flatten = spektral.layers.GlobalAvgPool()\n",
        "        self.fc1 = tf.keras.layers.Dense(512, activation=\"relu\") #512\n",
        "        self.fc2 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
        "        self.fc3 = tf.keras.layers.Dense(3, activation=\"softmax\")  # MNIST has 10 classes\n",
        "        self.a1 = tf.keras.layers.Activation('relu')\n",
        "    def call(self, inputs, training, mask = None):\n",
        "        x, a = inputs\n",
        "\n",
        "        x11 = self.L1(x)\n",
        "        x1  =self.conv2([x11, a])\n",
        "\n",
        "        x1 = self.b1(x1)\n",
        "        x1 = self.a1(x1)\n",
        "        x1 = self.d3(x1)\n",
        "        x1 = self.conv1([x1, a])\n",
        "        x1 = self.b2(x1)\n",
        "        x1 = self.d3(x1)\n",
        "\n",
        "        x1 = self.a1(x1)\n",
        "        x1 = self.flatten(x1)\n",
        "        output = self.fc1(x1)\n",
        "        if training:\n",
        "            output = self.d3(output)\n",
        "        output = self.fc2(output)\n",
        "        if training:\n",
        "                output = self.d4(output)\n",
        "        output = self.fc3(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, weight_decay=1e-3) \n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
        "\n",
        "# Training function\n",
        "@tf.function\n",
        "def train_on_batch(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True, mask = mask)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "        acc = tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(target, predictions))\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss, acc\n",
        "\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(loader):\n",
        "    step = 0\n",
        "    results = []\n",
        "    for batch in loader:\n",
        "        step += 1\n",
        "        inputs, target = batch\n",
        "        predictions = model(inputs, training=False)\n",
        "        target = tf.one_hot(tf.squeeze(target), depth=3)\n",
        "        loss = loss_fn(target, predictions)\n",
        "        acc = tf.reduce_mean(tf.keras.metrics.categorical_accuracy(target, predictions))\n",
        "        predictions_np = np.array(predictions)\n",
        "        predictions_np = np.argmax(predictions_np, axis=1)\n",
        "        target = np.argmax(target.numpy(), axis=1)\n",
        "        f1 = f1_score(target, predictions_np, average='macro')\n",
        "        results.append((loss, acc, f1, len(target)))  # Keep track of batch size\n",
        "        if step == loader.steps_per_epoch:\n",
        "            results = np.array(results)\n",
        "            return np.average(results[:, :-1], 0, weights=results[:, -1])\n",
        "\n",
        "\n",
        "# Setup training\n",
        "best_tr_loss = 999999\n",
        "current_patience = patience\n",
        "step = 0\n",
        "\n",
        "\n",
        "loader_tr = spektral.data.MixedLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
        "# Training loop\n",
        "results_tr = []\n",
        "epoch = 0\n",
        "for batch in loader_tr:\n",
        "    epoch +=1\n",
        "    step += 1\n",
        "    \n",
        "    # Training step\n",
        "    inputs, target = batch\n",
        "    inputs, target = batch\n",
        "    with tf.GradientTape() as tape:\n",
        "        mask = tf.ones((batch_size, 56), dtype=tf.float32)\n",
        "        predictions = model(inputs, training=True)\n",
        "        target = tf.one_hot(tf.squeeze(target), depth=3)\n",
        "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
        "        acc = tf.reduce_mean(tf.keras.metrics.categorical_accuracy(target, predictions))\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    results_tr.append((loss, acc, len(target)))\n",
        "\n",
        "    if step%loader_tr.steps_per_epoch==0:\n",
        "        np.random.shuffle(data_tr)\n",
        "        loader_tr = spektral.data.MixedLoader(data_tr, batch_size=batch_size, epochs=epochs)\n",
        "        if results_tr[0] < best_tr_loss:\n",
        "            best_tr_loss = results_tr[0]\n",
        "            current_patience = patience\n",
        "            results_te = evaluate(loader_te)\n",
        "        else:\n",
        "            current_patience -= 1\n",
        "            if current_patience == 0:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "        \n",
        "        results_tr = np.array(results_tr)\n",
        "        results_tr = np.average(results_tr[:, :-1], 0, weights=results_tr[:, -1])\n",
        "\n",
        "        # Print results\n",
        "        print(\n",
        "            \"Train loss: {:.4f}, acc: {:.4f} | \"\n",
        "            \"Test loss: {:.4f}, acc: {:.4f}, f1: {:.4f}\".format(\n",
        "                *results_tr, *results_te\n",
        "            )\n",
        "        )\n",
        "\n",
        "        results_tr = []"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
